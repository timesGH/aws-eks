name: Manual EKS Deploy/Destroy

# ONLY manual trigger, no code change triggers
on:
  workflow_dispatch:
    inputs:
      clusterName:
        description: 'Name of the EKS cluster'
        required: true
        default: 'my_eks_cluster'
      awsRegion:
        description: 'AWS Region'
        required: true
        default: 'us-east-1'
      action:
        description: 'Action to perform'
        required: true
        default: 'deploy'
        type: choice
        options:
          - deploy
          - remove

jobs:
  deploy-job:
    runs-on: ubuntu-latest
    if: github.event.inputs.action == 'deploy'
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ github.event.inputs.awsRegion }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
      
      - name: Create Terraform variables
        working-directory: Terraform
        run: |
          cat > terraform.auto.tfvars << EOF
          region = "${{ github.event.inputs.awsRegion }}"
          cluster_name = "${{ github.event.inputs.clusterName }}"
          EOF
      
      - name: Terraform Init
        working-directory: Terraform
        run: terraform init -upgrade
      
      - name: Terraform Apply
        working-directory: Terraform
        run: terraform apply -auto-approve
      
      - name: Update kubeconfig
        run: |
          aws eks --region ${{ github.event.inputs.awsRegion }} update-kubeconfig \
            --name ${{ github.event.inputs.clusterName }}
      
      - name: Wait for cluster to be ready
        run: sleep 60
      
      - name: Simple post-deploy
        run: |
          echo "Kubeconfig updated successfully!"
          echo "EKS cluster ${{ github.event.inputs.clusterName }} has been created!"
          echo "To connect to your cluster, run:"
          echo "aws eks --region ${{ github.event.inputs.awsRegion }} update-kubeconfig --name ${{ github.event.inputs.clusterName }}"

  destroy-job:
    runs-on: ubuntu-latest
    if: github.event.inputs.action == 'remove'
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ github.event.inputs.awsRegion }}
      
      - name: Clean up Kubernetes resources and Load Balancers
        run: |
          echo "üßπ Starting comprehensive cleanup process..."
          
          # Try to connect to the cluster first
          if aws eks describe-cluster --name ${{ github.event.inputs.clusterName }} --region ${{ github.event.inputs.awsRegion }} &> /dev/null; then
            echo "‚úÖ Cluster exists, connecting and cleaning up Kubernetes resources..."
            
            # Update kubeconfig
            aws eks --region ${{ github.event.inputs.awsRegion }} update-kubeconfig --name ${{ github.event.inputs.clusterName }}
            
            # Get all LoadBalancer services before deleting them
            echo "üîç Finding LoadBalancer services..."
            SERVICES=$(kubectl get services --all-namespaces -o jsonpath='{range .items[?(@.spec.type=="LoadBalancer")]}{.metadata.namespace}{"|"}{.metadata.name}{"\n"}{end}' || echo "")
            
            if [ ! -z "$SERVICES" ]; then
              echo "üìã Found LoadBalancer services:"
              echo "$SERVICES"
              
              # Delete each LoadBalancer service
              echo "$SERVICES" | while IFS='|' read -r namespace service_name; do
                if [ ! -z "$service_name" ]; then
                  echo "üóëÔ∏è  Deleting service $service_name in namespace $namespace..."
                  kubectl delete service "$service_name" -n "$namespace" --ignore-not-found=true
                fi
              done
              
              # Wait for Load Balancers to be cleaned up
              echo "‚è≥ Waiting 60 seconds for AWS Load Balancers to be deleted..."
              sleep 60
            fi
            
            # Delete all deployments, services, and other resources
            echo "üóëÔ∏è  Deleting all Kubernetes resources..."
            kubectl delete all --all --all-namespaces --ignore-not-found=true || true
            
            # Wait additional time for cleanup
            echo "‚è≥ Waiting additional 30 seconds for cleanup..."
            sleep 30
          else
            echo "‚ö†Ô∏è  Cluster doesn't exist, skipping Kubernetes cleanup"
          fi
      
      - name: Force cleanup any remaining Load Balancers
        run: |
          echo "üîç Checking for any remaining Load Balancers..."
          
          # Find Load Balancers by cluster tag
          LB_ARNS=$(aws elbv2 describe-load-balancers --output text --query "LoadBalancers[].LoadBalancerArn" || echo "")
          
          if [ ! -z "$LB_ARNS" ]; then
            for LB_ARN in $LB_ARNS; do
              # Check if this Load Balancer has the cluster tag
              TAGS=$(aws elbv2 describe-tags --resource-arns "$LB_ARN" --output text --query "TagDescriptions[0].Tags[?Key=='kubernetes.io/cluster/${{ github.event.inputs.clusterName }}'].Value" || echo "")
              
              if [ ! -z "$TAGS" ]; then
                echo "üóëÔ∏è  Force deleting Load Balancer: $LB_ARN"
                aws elbv2 delete-load-balancer --load-balancer-arn "$LB_ARN" || true
              fi
            done
          fi
          
          # Also check for classic Load Balancers (ELB v1)
          CLB_NAMES=$(aws elb describe-load-balancers --output text --query "LoadBalancerDescriptions[].LoadBalancerName" || echo "")
          
          if [ ! -z "$CLB_NAMES" ]; then
            for CLB_NAME in $CLB_NAMES; do
              # Check if this Classic Load Balancer has the cluster tag
              TAGS=$(aws elb describe-tags --load-balancer-names "$CLB_NAME" --output text --query "TagDescriptions[0].Tags[?Key=='kubernetes.io/cluster/${{ github.event.inputs.clusterName }}'].Value" || echo "")
              
              if [ ! -z "$TAGS" ]; then
                echo "üóëÔ∏è  Force deleting Classic Load Balancer: $CLB_NAME"
                aws elb delete-load-balancer --load-balancer-name "$CLB_NAME" || true
              fi
            done
          fi
          
          echo "‚è≥ Waiting for Load Balancers to finish deleting..."
          sleep 30
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
      
      - name: Create Terraform variables
        working-directory: Terraform
        run: |
          cat > terraform.auto.tfvars << EOF
          region = "${{ github.event.inputs.awsRegion }}"
          cluster_name = "${{ github.event.inputs.clusterName }}"
          EOF
      
      - name: Terraform Init
        working-directory: Terraform
        run: terraform init -upgrade
      
      - name: Terraform Destroy
        working-directory: Terraform
        run: terraform destroy -auto-approve || true
      
      - name: Final cleanup verification
        run: |
          echo "üîç Performing final verification..."
          
          # Check if cluster still exists
          if aws eks describe-cluster --name ${{ github.event.inputs.clusterName }} --region ${{ github.event.inputs.awsRegion }} &> /dev/null; then
            echo "‚ö†Ô∏è  Cluster still exists, attempting force deletion..."
            aws eks delete-cluster --name ${{ github.event.inputs.clusterName }} --region ${{ github.event.inputs.awsRegion }} || true
          else
            echo "‚úÖ Cluster successfully deleted"
          fi
          
          # Check for any remaining Load Balancers one more time
          REMAINING_LBS=$(aws elbv2 describe-load-balancers --output text --query "LoadBalancers[].LoadBalancerArn" | wc -w || echo "0")
          if [ "$REMAINING_LBS" -gt 0 ]; then
            echo "‚ö†Ô∏è  $REMAINING_LBS Load Balancer(s) still exist - they may still be deleting"
          else
            echo "‚úÖ No Load Balancers found"
          fi
          
          echo "üéâ Cleanup process completed!"
